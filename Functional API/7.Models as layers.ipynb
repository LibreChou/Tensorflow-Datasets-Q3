{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models as layers\n",
    "1 pora pretrained-model uthake layer ke tarah use kerley.\n",
    "\n",
    "Ismai bhe layer sharing ke tarah layers ke sath us wakt ke weights bhe atay hai(use hote hai).\n",
    "\n",
    "Calling an instance, whether it’s a layer\n",
    "instance or a model instance, will always reuse the existing learned representations of\n",
    "the instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This means you can call a model on an input tensor and retrieve an output tensor:\n",
    "y = model(x) # for eg this is a model\n",
    "\n",
    "#If the model has multiple input tensors and multiple output tensors, it should be called with a list of tensors:\n",
    "y1, y2 = model([x1, x2])\n",
    "\n",
    "# When you call a model instance, you’re reusing the weights of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One simple practical example of what you can build by reusing a model instance is\n",
    "a vision model that uses a dual camera as its input: two parallel cameras, a few centimeters (one inch) apart. Such a model can perceive depth, which can be useful in\n",
    "many applications. You shouldn’t need two independent models to extract visual features from the left camera and the right camera before merging the two feeds.\n",
    "Such low-level processing can be shared across the two inputs: that is, done via layers\n",
    "that use the same weights and thus share the same representations. Here’s how you’d\n",
    "implement a Siamese vision model (shared convolutional base) in Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import applications\n",
    "from keras import Input\n",
    "\n",
    "# The base image-processing model is the Xception network (convolutional base only)\n",
    "xception_base = applications.Xception(weights=None,include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_input = Input(shape=(250, 250, 3))  #The inputs are 250 × 250 RGB images.\n",
    "right_input = Input(shape=(250, 250, 3)) #Calls the same vision model twice\n",
    "\n",
    "left_features = xception_base(left_input)\n",
    "right_input = xception_base(right_input)\n",
    "\n",
    "merged_features = layers.concatenate([left_features, right_input], axis=-1)\n",
    "# The merged features contain information from the right visual feed and the left visual feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
