{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Imdb data from\n",
    " http://mng.bz/0tIo\n",
    "\n",
    "let’s collect the individual training reviews into a list of strings, one string per\n",
    "review. We’ll also collect the review labels (positive/negative) into a labels list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'test', 'train']\n",
      "['.DS_Store', 'neg', 'pos', 'urls_neg.txt', 'urls_pos.txt']\n"
     ]
    }
   ],
   "source": [
    "# Lets take a look at the downloaded dada\n",
    "\n",
    "import os\n",
    "\n",
    "print(os.listdir('aclImdb'))\n",
    "print(os.listdir('aclImdb/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB Dataset contain negative and positive reviews in text format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved Dedee Pfeiffer (is that spelled right?) in Cybil. Haven't seen her for awhile and forgot how much I missed her. I thought she did a great job in this. The supporting cast was pretty good too. In some angles, the daughter even looked like a young Nicole Kidman. The abductor was pretty creepy and the story generally had some good twists. The young boyfriend was a hottie. I thought the husband definitely had something to do with it for sure.<br /><br />Just got the Lifetime Movie Network for Christmas and am loving these movies. Kept my interest and I'll watch it again when they rerun it. Can anyone else recommend any similar movies to this? You can post on the board or send me a private email if you want. Thanks in advance. Aboutagirly.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"aclImdb/train/pos/268_8.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry everyone,,, I know this is supposed to be an \"art\" film,, but wow, they should have handed out guns at the screening so people could blow their brains out and not watch. Although the scene design and photographic direction was excellent, this story is too painful to watch. The absence of a sound track was brutal. The loooonnnnng shots were too long. How long can you watch two people just sitting there and talking? Especially when the dialogue is two people complaining. I really had a hard time just getting through this film. The performances were excellent, but how much of that dark, sombre, uninspired, stuff can you take? The only thing i liked was Maureen Stapleton and her red dress and dancing scene. Otherwise this was a ripoff of Bergman. And i'm no fan f his either. I think anyone who says they enjoyed 1 1/2 hours of this is,, well, lying.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"aclImdb/train/neg/10002_1.txt\", \"r\")\n",
    "print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the labels of the raw IMDB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With this entire code we just created labels for the neg and pos text\n",
    "\n",
    "import os\n",
    "imdb_dir = 'aclImdb'\n",
    "train_dir = os.path.join(imdb_dir, 'train') #train_dir = aclImdb\\train\n",
    "labels = []\n",
    "texts = []\n",
    "for label_type in ['neg', 'pos']:\n",
    "    dir_name = os.path.join(train_dir, label_type) #dir_name = aclImdb\\train\\neg\n",
    "                                                   #dir_name = aclImdb\\train\\pos\n",
    "        \n",
    "    for fname in os.listdir(dir_name):\n",
    "        if fname[-4:] == '.txt':                  # if file is text file\n",
    "            f = open(os.path.join(dir_name, fname),encoding=\"utf8\") # open it\n",
    "            texts.append(f.read())               # add in that empty texts list we made before\n",
    "            f.close()\n",
    "        if label_type == 'neg':                 # ager neg folder ka ho to 0 label\n",
    "            labels.append(0)                    # aur labels list mai daldo jo upper bnaya tha\n",
    "        else:\n",
    "            labels.append(1)                   # pos hoto 1 label deke labels folder mai daldo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the text of the raw IMDB data\n",
    "we’ll add the following twist:\n",
    "restricting the training data to the first 200 samples. So you’ll learn to classify movie\n",
    "reviews after looking at just 200 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100               # Cuts off reviews after 100 words\n",
    "training_samples = 200     # Trains on 200 samples\n",
    "validation_samples = 10000 # Validates on 10,000 samples\n",
    "max_words = 10000          # Considers only the top 10,000 words in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)    # max_words = 10000 \n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88582 unique tokens.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Found %s unique tokens.' % len(tokenizer.word_index))\n",
    "print()\n",
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad_sequences is used to ensure that all sequences in a list have the same length.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "data = pad_sequences(sequences, maxlen=maxlen)                    # maxlen = 100   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (25000, 100)\n",
      "Shape of label tensor: (25000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomizing data\n",
    "\n",
    "indices = np.arange(data.shape[0])   # same length ke range bnai \n",
    "np.random.shuffle(indices)           # uske position shuffle ke\n",
    "data = data[indices]                 # same shuffle position data ke bhe kerde\n",
    "labels = labels[indices]             # aur label ke bhe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 100)\n",
      "(200,)\n",
      "(10000, 100)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "# spliting data\n",
    "\n",
    "x_train = data[:training_samples]  # training_samples = 200 \n",
    "y_train = labels[:training_samples]\n",
    "x_val = data[training_samples: training_samples + validation_samples] # validation_samples = 10000\n",
    "y_val = labels[training_samples: training_samples + validation_samples]\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DOWNLOAD THE GLOVE WORD EMBEDDINGS\n",
    "Its a pretrained Word Embedding : \n",
    "https://nlp.stanford.edu/projects/glove\n",
    "\n",
    "It’s an 822 MB zip file called glove.6B.zip,\n",
    "containing 100-dimensional embedding vectors for 400,000 words (or nonword\n",
    "tokens).\n",
    "\n",
    "PREPROCESSING THE EMBEDDINGS\n",
    "Let’s parse the unzipped file (a .txt file) to build an index that maps words (as strings)\n",
    "to their vector representation (as number vectors)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the GloVe word-embeddings file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder contain : ['glove.6B.100d.txt', 'glove.6B.200d.txt', 'glove.6B.300d.txt', 'glove.6B.50d.txt']\n",
      "\n",
      "Printing first value :\n",
      "\n",
      " ['sandberger', '0.28365', '-0.6263', '-0.44351', '0.2177', '-0.087421', '-0.17062', '0.29266', '-0.024899', '0.26414', '-0.17023', '0.25817', '0.097484', '-0.33103', '-0.43859', '0.0095799', '0.095624', '-0.17777', '0.38886', '0.27151', '0.14742', '-0.43973', '-0.26588', '-0.024271', '0.27186', '-0.36761', '-0.24827', '-0.20815', '0.22128', '-0.044409', '0.021373', '0.24594', '0.26143', '0.29303', '0.13281', '0.082232', '-0.12869', '0.1622', '-0.22567', '-0.060348', '0.28703', '0.11381', '0.34839', '0.3419', '0.36996', '-0.13592', '0.0062694', '0.080317', '0.0036251', '0.43093', '0.01882', '0.31008', '0.16722', '0.074112', '-0.37745', '0.47363', '0.41284', '0.24471', '0.075965', '-0.51725', '-0.49481', '0.526', '-0.074645', '0.41434', '-0.1956', '-0.16544', '-0.045649', '-0.40153', '-0.13136', '-0.4672', '0.18825', '0.2612', '0.16854', '0.22615', '0.62992', '-0.1288', '0.055841', '0.01928', '0.024572', '0.46875', '0.2582', '-0.31672', '0.048591', '0.3277', '-0.50141', '0.30855', '0.11997', '-0.25768', '-0.039867', '-0.059672', '0.5525', '0.13885', '-0.22862', '0.071792', '-0.43208', '0.5398', '-0.085806', '0.032651', '0.43678', '-0.82607', '-0.15701']\n"
     ]
    }
   ],
   "source": [
    "# lets check the downloaded glove file\n",
    "\n",
    "import os\n",
    "print(\"Folder contain :\",os.listdir(\"glove.6B\"))\n",
    "print()\n",
    "\n",
    "# each file colntain word with its vextor lets check\n",
    "\n",
    "f = open('glove.6B/glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for i in f:\n",
    "    values = i.split()\n",
    "f.close()\n",
    "print(\"Printing first value :\\n\\n\",values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets put the same format in a dictionary\n",
    "\n",
    "glove_dir = 'glove.6B'                                                   # ?\n",
    "embeddings_index = {}\n",
    "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'),encoding=\"utf8\")\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "print()\n",
    "#print(embeddings_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you’ll build an embedding matrix that you can load into an Embedding layer. It\n",
    "must be a matrix of shape (max_words, embedding_dim), where each entry i contains\n",
    "the embedding_dim-dimensional vector for the word of index i in the reference word\n",
    "index (built during tokenization). Note that index 0 isn’t supposed to stand for any\n",
    "word or token—it’s a placeholder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the GloVe word-embeddings matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_gpu",
   "language": "python",
   "name": "conda_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
